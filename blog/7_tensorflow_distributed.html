<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>TensorFlow中的分布式 | Maureen</title>
    <meta name="description" content="Maureen&#39;s secret Garden">
    
    
    <link rel="preload" href="/assets/css/0.styles.760ca8ec.css" as="style"><link rel="preload" href="/assets/js/app.c7007cc2.js" as="script"><link rel="preload" href="/assets/js/3.d747811d.js" as="script"><link rel="preload" href="/assets/js/2.7c3b14ab.js" as="script"><link rel="preload" href="/assets/js/13.f8a9c860.js" as="script"><link rel="prefetch" href="/assets/js/10.3a39eca6.js"><link rel="prefetch" href="/assets/js/11.a55c4a00.js"><link rel="prefetch" href="/assets/js/12.9d34ec15.js"><link rel="prefetch" href="/assets/js/14.909998d3.js"><link rel="prefetch" href="/assets/js/15.9ee2e3dc.js"><link rel="prefetch" href="/assets/js/16.e566454c.js"><link rel="prefetch" href="/assets/js/4.d4863e86.js"><link rel="prefetch" href="/assets/js/5.014cc91d.js"><link rel="prefetch" href="/assets/js/6.d595216a.js"><link rel="prefetch" href="/assets/js/7.ae063ce4.js"><link rel="prefetch" href="/assets/js/8.950d680c.js"><link rel="prefetch" href="/assets/js/9.f9e9213f.js">
    <link rel="stylesheet" href="/assets/css/0.styles.760ca8ec.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar abcdefg"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Maureen</span></a> <!----> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"><div class="content default"><h1 id="tensorflow中的分布式"><a href="#tensorflow中的分布式" aria-hidden="true" class="header-anchor">#</a> TensorFlow中的分布式</h1> <p>最近在做多分类的任务，随着模型参数和训练数据的增多，单机的算力已经有些吃力，看着久久才打印出来的一条log，心中急切，便起了分布式训练的念头。TensorFlow自0.8版本开始支持分布式训练，下面分别从分布式训练结构、Distributed TensorFlow、一些实例来介绍（参考的文章在文末有列出）。</p> <h2 id="分布式训练的策略与结构"><a href="#分布式训练的策略与结构" aria-hidden="true" class="header-anchor">#</a> 分布式训练的策略与结构</h2> <h3 id="策略"><a href="#策略" aria-hidden="true" class="header-anchor">#</a> 策略</h3> <ol><li>模型并行</li></ol> <p>是指将模型分成若干部分，放到不同的机器上并行训练。</p> <p><strong>适用</strong>：规模巨大的模型，当一台机器的内存放不下整个模型的时候，可以考虑将模型拆分。</p> <p><strong>缺点</strong>：像CNN这种网络结构，层与层之间存在前后关系，且进行back propogation优化参数时，前面的层要依靠后面层的计算结果来计算梯度，强行拆分网络会影响模型训练效果。</p> <ol start="2"><li>数据并行</li></ol> <p>将训练数据拆分成多份，放到各个节点上，每个节点保存一个模型的副本，如下图所示[2]<img src="/images/data_parallel.png" alt="data_parallel"></p> <p><strong>适用</strong>：当训练数据集十分庞大，单个机器计算效率较低时，可以使用数据并行，也变相地增大了batch size。上图的示例中，共有256个节点，每个节点提供32张图片，batch size便是32 * 256 = 8092。而且这种策略鲁棒性更好，当一个节点挂掉时，模型的训练仍可继续。</p> <p><strong>缺点</strong>：数据并行可分为同步和异步，如图中所示。</p> <p><img src="/images/sync_and_async.png" alt="sync_and_async">[3]
从名字很好理解，同步就是每个节点分别计算mini batch的梯度<mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="3.584ex" height="1.62ex" viewBox="0 -716 1584 716" xmlns:xlink="http://www.w3.org/1999/xlink" style="vertical-align:0;"><defs><path id="MJX-1-TEX-N-394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"></path><path id="MJX-1-TEX-I-50" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-N-394"></use></g><g data-mml-node="mi" transform="translate(833, 0)"><use xlink:href="#MJX-1-TEX-I-50"></use></g></g></g></svg></mjx-container>，等到它们<strong>都在</strong>Parameter Server汇合后，更新参数，进入下一轮迭代。同步要求每个节点的算力最好相近，如果有一个非常差，就像木桶效应里最短的那块板一样，拖慢了整体进度。异步便是不用等待每个节点都传来<mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="3.584ex" height="1.62ex" viewBox="0 -716 1584 716" xmlns:xlink="http://www.w3.org/1999/xlink" style="vertical-align:0;"><defs><path id="MJX-1-TEX-N-394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"></path><path id="MJX-1-TEX-I-50" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-1-TEX-N-394"></use></g><g data-mml-node="mi" transform="translate(833, 0)"><use xlink:href="#MJX-1-TEX-I-50"></use></g></g></g></svg></mjx-container>，就更新参数，如果节点之间算力差异较大，快的节点已经更新好几个batch了，慢的节点还在哼哧哼哧地计算，就容易造成stale gradients ,模型最终不能回归到最优解。</p> <h3 id="结构"><a href="#结构" aria-hidden="true" class="header-anchor">#</a> 结构</h3> <ol><li>Parameter Server</li></ol> <p>在参数服务器结构中，节点被分为ps和worker。worker是干活的苦力，负责模型的计算、梯度的优化，ps负责存储、更新参数。每个iteration里，各个worker把算好的梯度传给ps，ps将它们整合、更新后，再广播给workers，结构如下图所示[2]。</p> <p><img src="/images/ps.png" alt="ps"></p> <ol start="2"><li>ring all-reduce</li></ol> <p>如下图所示[2],ring all-reduce结构中，没有像ps一样的中心节点，每个节点都是worker。当节点计算完梯度之后，便传递到与之相邻的下个节点，同时接收上个节点传来的梯度。对于一个有<mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink" style="vertical-align:0;"><defs><path id="MJX-2-TEX-I-4E" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-2-TEX-I-4E"></use></g></g></g></svg></mjx-container>个节点的环，进行<mjx-container jax="SVG" class="MathJax"><svg xmlns="http://www.w3.org/2000/svg" width="5.906ex" height="1.731ex" viewBox="0 -683 2610.4 765" xmlns:xlink="http://www.w3.org/1999/xlink" style="vertical-align:-0.186ex;"><defs><path id="MJX-3-TEX-I-4E" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-3-TEX-I-4E"></use></g><g data-mml-node="mo" transform="translate(1110.2, 0)"><use xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(2110.4, 0)"><use xlink:href="#MJX-3-TEX-N-31"></use></g></g></g></svg></mjx-container>次梯度传递后，就可以更新整个模型的参数了。ring all-reduce结构可以充分利用带宽，在前向计算的同时，后向计算梯度。</p> <p><img src="/images/ring_allreduce.png" alt="ring_allreduce"></p> <h2 id="tensorflow-中的分布式"><a href="#tensorflow-中的分布式" aria-hidden="true" class="header-anchor">#</a> TensorFlow 中的分布式</h2> <p>在TensorFlow中，参与计算的机器组成一个cluster，每台机器是一个server，一个server处理一个task，若干个task做同一件job（如ps，worker），这种结构如下图所示。一般用job name，task index来表示它们。</p> <p><img src="/images/cluster.png" alt="cluster"></p> <ol><li>首先，手动创建一个cluster，这个cluster里有2个job：worker与ps。另外，仅仅是为了演示才用ip和端口来创建cluster，生产环境下应当使用YARN、Kubernetes等cluster manager来创建。</li></ol> <div class="language-python extra-class"><pre class="language-python"><code>tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>ClusterSpec<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">&quot;local&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;localhost:2222&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;localhost:2223&quot;</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>ClusterSpec<span class="token punctuation">(</span><span class="token punctuation">{</span>
    <span class="token string">&quot;worker&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
        <span class="token string">&quot;worker0.example.com:2222&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;worker1.example.com:2222&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;worker2.example.com:2222&quot;</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">&quot;ps&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
        <span class="token string">&quot;ps0.example.com:2222&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;ps1.example.com:2222&quot;</span>
    <span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> FLAGS<span class="token punctuation">.</span>job_name <span class="token operator">==</span> <span class="token string">&quot;ps&quot;</span><span class="token punctuation">:</span>
    server<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">elif</span> FLAGS<span class="token punctuation">.</span>job_name <span class="token operator">==</span> <span class="token string">&quot;worker&quot;</span><span class="token punctuation">:</span>
</code></pre></div><ol start="2"><li>其次，在cluster上创建server</li></ol> <div class="language-python extra-class"><pre class="language-python"><code>server <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>Server<span class="token punctuation">(</span>cluster<span class="token punctuation">,</span> job_name<span class="token operator">=</span><span class="token string">&quot;worker&quot;</span><span class="token punctuation">,</span> task_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre></div><p>在TensorFlow中，client是一个很重要的概念，cluster和server仅搭建了分布式环境，client构建TensorFlow graph和Session，并与cluster通信。</p> <ol start="3"><li>开始训练</li></ol> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">with</span> tf<span class="token punctuation">.</span>Session<span class="token punctuation">(</span><span class="token string">&quot;grpc://worker1.example.com:2223&quot;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> sess<span class="token punctuation">:</span>
  <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>train_op<span class="token punctuation">)</span>
</code></pre></div><p>在分布式系统下，要指定Session的target，或者直接使用server.target。并行的方式可分为in-graph replication 和 between-graph replication。从名字可以一窥全貌：in-graph是指在每个worker上都有一个模型的副本，但使用同一个graph。这种方式比较危险，因为一旦一个worker挂了，训练便无法继续。<strong>between-graph</strong>是指每个worker都创建一个client，这个client一般还与task的主程序在同一进程中。各个client构建相同的Graph，但是参数还是放置在ps上。但是有这么多的client，参数的初始化和checkpoint的保存也比较麻烦，于是tf.train.MonitoredTrainingSession，可以指定chief worker，协调各个worker的训练，并完成模型初始化与恢复这些general operation。</p> <h1 id="参考资料"><a href="#参考资料" aria-hidden="true" class="header-anchor">#</a> 参考资料</h1> <p>[1]<a href="https://zhuanlan.zhihu.com/p/35083779" target="_blank" rel="noopener noreferrer">分布式TensorFlow入门教程<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>[2]<a href="https://www.oreilly.com/ideas/distributed-tensorflow" target="_blank" rel="noopener noreferrer">Distributed TensorFlow<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>[3]<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf" target="_blank" rel="noopener noreferrer">TensorFlow:
Large-Scale Machine Learning on Heterogeneous Distributed Systems<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></div><footer class="page-edit"><!----><div class="last-updated"><span class="prefix">Last Updated: </span><span class="time">7/8/2019, 8:26:04 PM</span></div></footer><!----></main></div><div class="global-ui"><!----></div></div>
    <script src="/assets/js/app.c7007cc2.js" defer></script><script src="/assets/js/3.d747811d.js" defer></script><script src="/assets/js/2.7c3b14ab.js" defer></script><script src="/assets/js/13.f8a9c860.js" defer></script>
  </body>
</html>
